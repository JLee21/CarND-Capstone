{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "suggestion for using TF Object Detection\n",
    "https://medium.com/@anthony_sarkis/self-driving-cars-implementing-real-time-traffic-light-detection-and-classification-in-2017-7d9ae8df1c58\n",
    "\n",
    "Google's Approach to traffic light\n",
    "https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37259.pdf\n",
    "\n",
    "SqueezeNet trained on ImageNet\n",
    "https://medium.freecodecamp.org/recognizing-traffic-lights-with-deep-learning-23dae23287cc\n",
    "\n",
    "Convert Caffe Models to TF\n",
    "https://github.com/ethereon/caffe-tensorflow\n",
    "\n",
    "Transfer Learning using Keras (blog)\n",
    "https://towardsdatascience.com/transfer-learning-using-keras-d804b2e04ef8\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So we can see the output of multiple variables in a single cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Datasets\n",
    " \n",
    "Change the download_dir in config.py\n",
    "\n",
    "---\n",
    "## Los Altos\n",
    "Contains a png images of real driving in Los Altos and the classification of and bounding box of traffic lights\n",
    "\n",
    "##### data\n",
    "./data/los-altos/train/\\*\\*/\\*.png\n",
    "##### labels\n",
    "./data/los-altos/train.yaml\n",
    "Labels are in yaml format\n",
    "```yaml\n",
    "  path: ./rgb/train/2015-10-05-16-02-30_bag/703062.png\n",
    "- boxes:\n",
    "  - {label: RedLeft, occluded: false, x_max: 536.2627074943, x_min: 527.9504961004,\n",
    "    y_max: 259.4134134066, y_min: 239.7337501584}\n",
    "  - {label: Red, occluded: false, x_max: 606.5768706269, x_min: 599.1649383548, y_max: 258.8548785478,\n",
    "    y_min: 239.7216236998}\n",
    "  - {label: Red, occluded: false, x_max: 663.2255031115, x_min: 655.7908822815, y_max: 258.9849821711,\n",
    "    y_min: 240.196122899}\n",
    "  - {label: Red, occluded: false, x_max: 832.5396369015, x_min: 820.5931243983, y_max: 277.8766194893,\n",
    "    y_min: 245.8794585057}\n",
    "  path: ./rgb/train/2015-10-05-16-02-30_bag/703270.png\n",
    "```\n",
    "---\n",
    "## Just Traffic Lights\n",
    "Only contains cropped images of Traffic Lights (png)\n",
    "##### data\n",
    "./data/just-traffic-lights / [ real | simulated ] / [ Green|NoTraficLight|Red|Yellow ] /\\*.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has apparently already been downloaded and unpacked.\n",
      "Data has apparently already been downloaded and unpacked.\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "import download\n",
    "\n",
    "for dataset in config.datasets:\n",
    "    download.maybe_download_and_extract(dataset, config.download_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " 'los-altos',\n",
       " 'dataset_additional_rgb.zip',\n",
       " 'dataset_train_rgb_bosch.zip',\n",
       " 'just-traffic-lights',\n",
       " 'los-altos.zip',\n",
       " 'tmp',\n",
       " 'tl_classifier_data.zip']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore data\n",
    "import os\n",
    "from glob import glob\n",
    "os.listdir('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = []\n",
    "for root, dirs, files in os.walk('./data/just-traffic-lights'):\n",
    "    for file in files:\n",
    "        image_paths.append(os.path.join(root,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Image_Size = namedtuple('Image_Size', 'x y ch')\n",
    "image_size = Image_Size(x=50, y=50, ch=3)\n",
    "num_classes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "972"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "334"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "551"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create validation train and test set\n",
    "image_paths = shuffle(image_paths)\n",
    "\n",
    "train_image_paths = image_paths[:-100]\n",
    "test_image_paths = image_paths[-100:]\n",
    "len(train_image_paths)\n",
    "len(test_image_paths)\n",
    "x = [i.split('/')[-2] for i in train_image_paths]\n",
    "len([i for i in x if i == \"Green\"])\n",
    "len([i for i in x if i == \"Yellow\"])\n",
    "len([i for i in x if i == \"Red\"])\n",
    "len([i for i in x if i == \"NoTrafficLight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/just-traffic-lights/real/NoTrafficLight/001053.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_indices(image_paths):\n",
    "    \"\"\"\n",
    "    0 = none\n",
    "    1 = green\n",
    "    2 = yellow\n",
    "    3 = red\n",
    "    \n",
    "    :return :\n",
    "    {'./data/just-traffic-lights/real/Green/001094.png': 1}\n",
    "    \"\"\"\n",
    "\n",
    "    ind_dict = {}\n",
    "    for img in image_paths:\n",
    "        if \"NoTrafficLight\" in img: \n",
    "            ind_dict[img] = 0\n",
    "        if \"Green\" in img: \n",
    "            ind_dict[img] = 1\n",
    "        if \"Yellow\" in img: \n",
    "            ind_dict[img] = 2\n",
    "        if \"Red\" in img: \n",
    "            ind_dict[img] = 3\n",
    "        if not ind_dict[img]:\n",
    "            print(img)\n",
    "            \n",
    "    return ind_dict\n",
    "\n",
    "def get_one_hot(image_paths, ver='keras'):\n",
    "    \n",
    "    ind = create_indices(image_paths)\n",
    "    ind = [ind for key,ind in ind.items()]\n",
    "    \n",
    "    # tf version\n",
    "    if ver=='tf':\n",
    "        one_hot = tf.one_hot(ind, num_classes)\n",
    "    \n",
    "    # keras version\n",
    "    if ver==\"keras\":\n",
    "        one_hot = keras.utils.to_categorical(ind, num_classes=num_classes)\n",
    "    \n",
    "    return one_hot\n",
    "\n",
    "get_one_hot(image_paths[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from scipy.ndimage import imread\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.,  1.],\n",
       "        [ 1.,  1.],\n",
       "        [ 1.,  1.]],\n",
       "\n",
       "       [[ 1.,  1.],\n",
       "        [ 1.,  1.],\n",
       "        [ 1.,  1.]],\n",
       "\n",
       "       [[ 1.,  1.],\n",
       "        [ 1.,  1.],\n",
       "        [ 1.,  1.]],\n",
       "\n",
       "       [[ 1.,  1.],\n",
       "        [ 1.,  1.],\n",
       "        [ 1.,  1.]]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(4, 3, 2)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 6, 5)\n",
      "[[[ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  1.  0.]\n",
      "  [ 0.  0.  1.  1.  0.]\n",
      "  [ 0.  0.  1.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  1.  0.]\n",
      "  [ 0.  0.  1.  1.  0.]\n",
      "  [ 0.  0.  1.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  1.  0.]\n",
      "  [ 0.  0.  1.  1.  0.]\n",
      "  [ 0.  0.  1.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  1.  0.]\n",
      "  [ 0.  0.  1.  1.  0.]\n",
      "  [ 0.  0.  1.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]]]\n"
     ]
    }
   ],
   "source": [
    "a = np.ones((4, 3, 2))\n",
    "a\n",
    "a.shape\n",
    "\n",
    "# npad is a tuple of (n_before, n_after) for each dimension\n",
    "npad = ((0, 0), (1, 2), (2, 1))\n",
    "b = np.pad(a, pad_width=npad, mode='constant', constant_values=0)\n",
    "\n",
    "print(b.shape)\n",
    "# (4, 6, 5)\n",
    "\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/topher/anaconda/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 7290000 into shape (972,150,150,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-171-fc0fe60fe2a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mtrain_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m972\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-171-fc0fe60fe2a5>\u001b[0m in \u001b[0;36mgenerator\u001b[0;34m(samples, batch_size, verbose)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'++'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# get one-hot-encode labels based on path name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 7290000 into shape (972,150,150,3)"
     ]
    }
   ],
   "source": [
    "def generator(samples, batch_size=1, verbose=False):\n",
    "    shuffle(samples)\n",
    "    num_samples = len(samples)\n",
    "    \n",
    "    for offset in range(0, num_samples, batch_size):\n",
    "        \n",
    "        # get a batch of image_paths\n",
    "        x_batch_paths = samples[offset : offset+batch_size]\n",
    "        x_batch = []\n",
    "        for img_path in x_batch_paths:\n",
    "            img = imread(img_path)\n",
    "            new_shape = resize(img, (50, 50)) \n",
    "            x_batch.append(new_shape)\n",
    "            \n",
    "        # reshape images to 4D: (batch_size, row, col, chs)\n",
    "        x_batch = np.array(x_batch)\n",
    "        if verbose: print('++', x_batch.size)\n",
    "        x_batch = x_batch.reshape(batch_size, 150, 150, image_size.ch)\n",
    "        \n",
    "        # get one-hot-encode labels based on path name\n",
    "        y_batch = get_one_hot(x_batch_paths)\n",
    "\n",
    "        if not len(y_batch) == len(x_batch):\n",
    "            print('Wrong Match ', len(y_batch), len(x_batch))\n",
    "        yield (x_batch, y_batch)\n",
    "            \n",
    "train_generator = generator(image_paths, 972, verbose=False)\n",
    "\n",
    "x,y = next(train_generator)\n",
    "x.shape\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    horizontal_flip = True,\n",
    "    fill_mode = \"nearest\",\n",
    "    shear_range = 0.5,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    rotation_range=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_datagen = ImageDataGenerator(\n",
    "rescale = 1./255,\n",
    "horizontal_flip = False,\n",
    "fill_mode = \"nearest\",\n",
    "zoom_range = 0.0,\n",
    "width_shift_range = 0.0,\n",
    "height_shift_range=0.0,\n",
    "rotation_range=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/topher/anaconda/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/just-traffic-lights/real/NoTrafficLight/001053.png\n",
      "./data/just-traffic-lights/simulator/NoTrafficLight/001111.png\n",
      "./data/just-traffic-lights/real/NoTrafficLight/001060.png\n",
      "./data/just-traffic-lights/real/NoTrafficLight/000066.png\n",
      "./data/just-traffic-lights/real/NoTrafficLight/001216.png\n",
      "./data/just-traffic-lights/real/NoTrafficLight/001223.png\n",
      "./data/just-traffic-lights/real/NoTrafficLight/000088.png\n",
      "./data/just-traffic-lights/simulator/NoTrafficLight/001108.png\n",
      "./data/just-traffic-lights/real/NoTrafficLight/001464.png\n",
      "./data/just-traffic-lights/real/NoTrafficLight/000083.png\n",
      "./data/just-traffic-lights/real/NoTrafficLight/001342.png\n",
      "./data/just-traffic-lights/real/NoTrafficLight/001327.png\n",
      "./data/just-traffic-lights/simulator/NoTrafficLight/001103.png\n",
      "./data/just-traffic-lights/real/NoTrafficLight/001466.png\n",
      "./data/just-traffic-lights/simulator/NoTrafficLight/001119.png\n",
      "./data/just-traffic-lights/real/NoTrafficLight/001070.png\n",
      "./data/just-traffic-lights/real/NoTrafficLight/001079.png\n",
      "./data/just-traffic-lights/simulator/NoTrafficLight/001115.png\n",
      "./data/just-traffic-lights/real/NoTrafficLight/001011.png\n",
      "./data/just-traffic-lights/real/NoTrafficLight/001237.png\n",
      "./data/just-traffic-lights/real/NoTrafficLight/000072.png\n",
      "./data/just-traffic-lights/real/NoTrafficLight/001056.png\n",
      "./data/just-traffic-lights/simulator/NoTrafficLight/001117.png\n",
      "./data/just-traffic-lights/real/NoTrafficLight/001319.png\n",
      "./data/just-traffic-lights/real/NoTrafficLight/000091.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(972, 150, 150, 3)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train = next(generator(train_image_paths, batch_size=972))\n",
    "x_test, y_test = next(generator(test_image_paths, batch_size=100))\n",
    "\n",
    "train_gen = train_datagen.flow(x=x_train, y=y_train)\n",
    "test_gen = train_datagen.flow(x=x_test, y=y_test)\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_17 (Conv2D)           (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 146, 146, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 73, 73, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 73, 73, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 341056)            0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               43655296  \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 43,675,204\n",
      "Trainable params: 43,675,204\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Resize())\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(150, 150, 3)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "64/64 [==============================] - 163s 3s/step - loss: 0.3051 - acc: 0.8952 - val_loss: 0.2625 - val_acc: 0.9000\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 162s 3s/step - loss: 0.2555 - acc: 0.9136 - val_loss: 0.2176 - val_acc: 0.9100\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 163s 3s/step - loss: 0.2430 - acc: 0.9184 - val_loss: 0.2083 - val_acc: 0.9200\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 161s 3s/step - loss: 0.1967 - acc: 0.9318 - val_loss: 0.2119 - val_acc: 0.9200\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 170s 3s/step - loss: 0.1771 - acc: 0.9396 - val_loss: 0.2121 - val_acc: 0.9300\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 168s 3s/step - loss: 0.1768 - acc: 0.9431 - val_loss: 0.1834 - val_acc: 0.9300\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 162s 3s/step - loss: 0.1437 - acc: 0.9533 - val_loss: 0.1831 - val_acc: 0.9400\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 159s 2s/step - loss: 0.1454 - acc: 0.9531 - val_loss: 0.1577 - val_acc: 0.9400\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 159s 2s/step - loss: 0.1347 - acc: 0.9551 - val_loss: 0.1762 - val_acc: 0.9600\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 161s 3s/step - loss: 0.1255 - acc: 0.9533 - val_loss: 0.2142 - val_acc: 0.9600\n"
     ]
    }
   ],
   "source": [
    "# samples_per_epoch = len(images_paths)\n",
    "nb_val_samples = 10\n",
    "nb_max_epoch = 10\n",
    "steps_per_epoch = 128\n",
    "\n",
    "hist = model.fit_generator(\n",
    "    generator=train_gen, \n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=nb_max_epoch,\n",
    "    validation_data=test_gen,\n",
    "    use_multiprocessing=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZkAAAD5CAYAAAD1JkggAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkBERkcgoZEREJDIKGRERicz/Bwcro8hrXP98AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11df06898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = hist.history['acc']\n",
    "loss = hist.history['loss']\n",
    "plt.plot(loss)\n",
    "plt.plot(acc)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
